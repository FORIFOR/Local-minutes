# ============================================================
# M4-Meet 環境設定ファイル (.env.example)
# ============================================================
# このファイルをコピーして .env を作成してください
# 'npm run setup' を実行すると自動で .env が生成されます

# --- モデル格納ディレクトリ ---
# セットアップスクリプトでダウンロードしたモデルの場所
M4_MODELS_DIR=$HOME/m4-meet-models
M4_ASR_DIR=$HOME/m4-meet-models/sherpa_jp
M4_DIAR_DIR=$HOME/m4-meet-models/diar

# --- 翻訳/TTS (オプション) ---
# 翻訳とTTSが不要な場合はこれらの行をコメントアウト
M4_CT2_DIR=$HOME/m4-meet-models/ct2_m2m100_418m
M4_TTS_VOICE=$HOME/m4-meet-models/tts/piper/ja_JP-lessac-medium.onnx

# --- LLM 設定 (要約機能) ---
# 推奨: Ollama (ローカルで動作するOpenAI互換API)
M4_LLM_PROVIDER=ollama
M4_OLLAMA_BASE=http://127.0.0.1:11434
M4_OLLAMA_MODEL=qwen2.5:7b-instruct
M4_OLLAMA_TEMPERATURE=0.3
M4_OLLAMA_MAX_TOKENS=768
M4_OLLAMA_TIMEOUT=180

# 代替: llama.cpp を使用する場合 (Ollamaの代わり)
# M4_LLM_PROVIDER=  # 空にするか未設定
# M4_LLM_BIN=llama-cli
# M4_LLM_MODEL=$HOME/m4-meet-models/llm/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf

# --- サーバーポート設定 ---
PORT_BACKEND=8000
PORT_FRONTEND=5173

# --- ログとデータディレクトリ ---
LOG_DIR=backend/data

# --- ASR/バッチ処理のランタイム設定 ---
M4_ASR_KIND=sense-voice-offline
M4_ASR_LIVE=1
M4_BATCH_WHISPER=off
M4_BATCH_TRANSLATE=off
M4_BATCH_SUMMARY=off
